# 一些问题

---

## 1. 协程怎么封装的？

我们用的是 C++20 的协程，自己封了一个 `task<T>` 类型，底层是 `std::coroutine_handle` 和自定义的 `task_promise`。

封装时主要定了这几件事：一是**默认初始挂起**，协程被创建后不会马上跑，要等有人 `co_await` 或显式 resume 才执行；二是**结束时的行为**，协程跑完后会挂起而不是直接销毁，如果有 caller 在等就把执行权还给它，如果是顶层协程就检查异常，有就报错、没有就安静结束；三是**被 co_await 时的行为**，如果我已经完成，调用方不用挂起直接继续，如果没完成，调用方挂起并记下「在等我」，把执行权交给我；四是**恢复时的行为**，我完成后 caller 被恢复，从我这取结果，如果有异常就在取结果时重新抛出。

实现上，`task` 持有一个 `coroutine_handle<task_promise<T>>`，promise 里放了 caller 句柄、完成回调（给 spawn 和 block_on 的 tracker 用）、以及 final_awaiter。final_awaiter 在协程结束时先触发完成回调，再根据有没有 caller 决定返回 caller 还是返回空。`task` 自己实现了 `operator co_await`，返回的 awaiter 里：await_ready 看是否已经完成，await_suspend 里把当前协程记成 callee 的 caller 并 resume callee，await_resume 里取返回值或抛异常。这样业务侧就可以直接 `co_await some_task()`，不用碰 handle。

---

## 2. 并发接口怎么设计的？

并发入口主要是两个：**spawn** 和 **block_on / wait_all**。

**spawn** 是「把协程丢进运行时去跑」，不阻塞当前线程。接口就是 `spawn(task<T>&&)`，内部把 task 的 handle 取出来，如果当前在 block_on 的上下文中还会给这个 handle 挂上完成回调并往 block_on 的 tracker 里注册，这样 block_on 能等到所有子 spawn 完成。然后根据当前是不是 worker 线程决定推本地队列还是全局队列，由 worker 循环取出来执行。所以 spawn 是「提交即返回」，适合「 fire-and-forget」或「我后面用 block_on 一起等」的场景。

**block_on** 是「在当前线程阻塞，直到传入的 task 跑完并返回结果」。实现上会建一个 block_on_tracker、一个 result_slot，用外壳协程包一层用户 task，把外壳投到全局队列，主线程在 tracker 上 wait，等主协程和所有在 block_on 期间 spawn 出去的子协程都完成后再从 slot 里取返回值或异常。**wait_all** 是「并行跑多个 task，阻塞直到全部完成，返回一个 tuple」。做法是预注册多个 pending、每个 task 一个外壳协程和 result_slot，一起投到队列，等 tracker 上全部完成后逐个从 slot 取结果拼成 tuple。所以并发模型是：spawn 负责「丢任务」，block_on/wait_all 负责「收结果 + 等待整棵任务树」，没有再做额外的「线程池 submit 返回 future」那一套，和协程的 task 是同一套类型。

---

## 3. block_on 的分组跟踪怎么实现的？

block_on 要等的不只是你直接传进去的那一个 task，还有它里面 spawn 出去的所有子协程，所以需要「分组跟踪」：知道哪些协程属于当前这次 block_on，等它们全完成再解除阻塞。

我们用的是 **block_on_tracker** 加 **完成回调**。每次调 block_on 时创建一个 tracker，里面有一个原子变量 pending_count 和一个 completion_signal。主 task 算一个，所以先 register_subtask 一次；然后把用户 task 包进外壳协程 block_on_coro，外壳里把 current_tracker 设成这个 tracker，co_await 用户 task，取到结果或异常后写入 result_slot，最后调 tracker->complete_subtask() 表示主 task 完成。关键是在 spawn 里：如果当前线程的 current_tracker 非空，说明是在某次 block_on 的上下文中，就会给即将被 push 的 handle 的 promise 挂上 _on_complete 回调，参数是 tracker；协程在 final_awaiter 里结束时就会调这个回调，回调里调 tracker->complete_subtask()。同时 spawn 时还会 tracker->register_subtask()，把「待完成数」加一。这样每 spawn 一个就加一，每完成一个（主 task 或子协程）就减一，减到零时在 complete_subtask 里 mark_ready 那个 completion_signal，主线程在 tracker.wait_all_done() 里等这个 signal，就实现了「主协程 + 所有子 spawn 都完成才返回」。current_tracker 是 thread_local，在 block_on_coro 里设为当前 tracker，在 worker 上跑子协程时通过 spawn 时的注册把 tracker 和完成回调绑在 promise 上，所以不需要在子协程里再访问 thread_local，分组关系是靠「谁在 block_on 里 spawn 的」自然形成的。

---

## 4. 异步运行时怎么设计的？

运行时是「多 worker 线程 + 每线程一个 io_uring + 任务队列」的结构。

有一个 RuntimePoller，里面是多个 Worker，每个 Worker 绑定一个线程，有一个 IoEngine（封装 io_uring）、一个本地任务队列、以及和别的 worker 共享的全局队列等。Worker 的 run() 是一个大循环：先 tick 更新时间戳，再 periodic 做周期任务（比如检查关闭标志、驱动 IO 引擎），然后优先从本地队列拿下一个任务，没有就去别的 worker 偷任务（task_steal），再没有就 drive_io 处理一次 io_uring 的提交和收割，最后没活干就 sleep。任务来源有两类：一是用户通过 spawn 或 block_on 推过来的协程 handle，二是 IO 完成里通过 io_user_data 里存的 handle 恢复的协程。所以运行时的职责就是「在多个线程上调度这些 handle，并在有 IO 时提交/收割 io_uring、在完成时 resume 对应协程」。

和协程的衔接是：task 的 await_suspend 里要么把当前 handle 交给 IO（通过 io_user_data），要么把被 resume 的 callee 的 handle 推回队列；worker 从队列里取出来的就是这些 handle，执行一次 resume，协程跑一段直到再挂起或结束。没有用「调度器对象 + 提交闭包」那种方式，而是「队列里就是 coroutine_handle，跑完一段再决定下一个是谁」，这样和 C++20 协程的对称转移、零堆分配 spawn 都能对上。

---

## 5. 异步 IO 怎么和协程结合的？

异步 IO 用的是 io_uring，和协程的结合方式是「IO 操作封装成 awaiter，协程在 await_suspend 里挂起并提交 SQE，完成时在 CQE 里 resume」。

具体来说，像 read、write、accept 这些操作都有一个 IORegistrantAwaiter 的封装：在构造函数里从 current_uring 拿一个 SQE，用 io_uring_prep_* 填好，然后把 io_user_data 设到 SQE 上，user_data 里存了当前协程的 handle（在 await_suspend 里填的）以及可选的超时等。await_ready 看 SQE 是否拿到（拿不到说明出错，不挂起）；await_suspend 里把调用者的 handle 放进 user_data，然后 submit 一次 io_uring，协程就挂起了。worker 在 drive_io 里会收割 CQE，从 user_data 里取出 handle，要么 resume 这个 handle（成功），要么根据错误码做超时取消等处理。所以协程侧就是 `co_await stream.read(buf)`，底层变成「提交一条 read SQE、挂起；某时刻 CQE 回来、resume」，业务不用写回调。超时是通过在 user_data 里带 deadline，定时器或 IO 引擎在合适时机取消这条 SQE 并 resume 协程报错来实现的。这样「异步 IO」和「协程挂起/恢复」就统一在 io_uring 的提交/完成和 promise 的 await_suspend/resume 这一套上了。

---

## 6. 同步原语你怎么设计实现的？

我们做了协程可用的 mutex 和 channel，都是「等不到就挂起协程、不占线程」的设计。

**mutex**：状态用一个原子变量表示，要么是「未锁」，要么是「已锁」或「已锁且后面挂了一串等待者」。等待者是一个 Awaiter 链表，用头插法（LIFO）。lock() 返回一个 Awaiter，co_await mutex.lock() 时：await_suspend 里先尝试 CAS 抢锁，抢到就不挂起（返回 false），抢不到就把当前 Awaiter 头插进链表并挂起（返回 true）。unlock 时看链表是否为空，空就把状态置成未锁，不空就取头一个等待者 resume，相当于把锁交给它。所以没有自旋、没有阻塞线程，等锁的协程只是不占队列，被 resume 后才继续参与调度。

**channel**（若有时）：思路类似「有界队列 + 发送/接收两侧的等待队列」，发不出去或拿不到就挂起协程，在对方或条件满足时 resume。具体实现要看代码里的 ring_buffer 和 sender/receiver。

整体上同步原语的设计原则是：在协程里用 co_await 等待，底层是「把当前 handle 挂到某个等待结构上，不占用 worker」，等条件满足再 resume，这样不会把线程卡住，和运行时的多 worker 调度是兼容的。

---

## 7. 你的定时器怎么实现的？

定时器是**多级时间轮**，每个 worker 一个 Timer 实例（thread_local），和 io_uring 的超时、协程的 sleep 一起用。

时间轮是分层结构：最底层一档是「一格 1ms、一圈 64 格」这种，上面每一层的一格管下面一层整圈的时间跨度，用 variant 存不同层级的 wheel，按时间跨度升降级。Timer 记录一个启动时间 _start，用 elapsed_ms() 算「从启动到现在过了多少毫秒」，任务进来时按「到期时间相对当前时刻的间隔」算该放进哪一层哪一槽。add_task 把 TimerTask（要么是 coroutine_handle，要么是 io_user_data 用于取消 IO）挂到对应槽位；poll 在 worker 的 periodic 里被调用，推进当前时间，把到期的槽位里的任务取出来，该 resume 的 resume，该取消 IO 的交给 io 引擎取消。这样 sleep、超时都是在「时间到了就 resume 或取消」，不需要额外线程，和 worker 主循环 tick + periodic 是绑在一起的。

---

## 8. HTTP 模块用到哪些库，如何进行封装的？

HTTP 模块主要用了 **llhttp**（解析/生成 HTTP/1.x）和 **nghttp2**（HTTP/2 帧和会话），再往上自己做了语义层和门面。

**llhttp**：用在 H1 的 server 和 client session 里。请求/响应都是「字节流 → llhttp 解析 → 回调里聚合到 _current_* 和 body → on_message_complete 时拼成 HttpRequest 或 HttpResponse」。我们只把 llhttp 当解析器用，回调里不做事务逻辑，只填 buffer 和状态，完整消息到了再构造 HttpRequest/HttpResponse 交给业务。序列化那边（比如响应 status line、headers、body）是自己按 HTTP/1.1 格式拼的，没有用 llhttp 的序列化。

**nghttp2**：用在 H2 的 server 和 client session 里。通过 nghttp2 的 session API 提交请求、收帧，回调里只做「状态聚合」：比如 server 侧把 :method、:path、headers、data 帧内容填到 StreamRequest 里，等 headers_complete 且 body_complete 再推入 pending_requests，由会话主循环取出来调 handler、再 submit_response。发送时把 HttpResponse 转成 nghttp2_nv 和可选的 data provider，用 nghttp2_submit_response 等提交，然后 send_data 把 nghttp2_session_mem_send2 产出的字节写回 TcpStream。错误码用 nghttp2_error_to_faio 统一映射到 faio::Error。

**封装原则**：业务只看到 HttpRequest、HttpResponse、HttpRouter、HttpServer/HttpStream，不直接碰 llhttp 或 nghttp2 的 API；协议层（v1/v2 的 session）负责「字节 ↔ 语义对象」和「会话状态机」，门面负责监听、协议探测、每连接起协程、把 handler 传给 session。所以第三方库都在 detail 里，对外是统一的类型和异步 task 接口。

---


### 9：路由和中间件在框架里是怎么实现的？

 路由和中间件都是「先注册、后按顺序执行」这一套，没有用反射。

中间件这边，我们有一个 `HttpMiddleware` 类型，本质就是一个 `std::function`，入参是 `HttpRequest`，返回的是 `task<HttpMiddlewareResult>`。`HttpMiddlewareResult` 里要么是「继续往下走」的 `next()`，要么是「直接返回这个响应」的 `respond(response)`。在 `dispatch` 里会先按注册顺序依次执行所有中间件；只要有一个返回 `handled() == true`，就直接把它的 `response()` 返回给客户端，后面的路由和 fallback 都不再走了。所以中间件是靠「顺序 + 返回值是否 handled」来实现的，没有动态发现，就是显式 `router.use(middleware)` 往链表里塞。

路由这边分静态和动态。静态路由就是按「方法 + 去掉 query 的 path」存成 map：每种 HTTP 方法一个 `unordered_map<string, HttpHandler>`，再加上一个不区分方法的 `_any_method_routes`。匹配时先用请求的 method 找到对应的 map，再用 `strip_query(req.path())` 得到的 path 去查，查到就调对应的 handler，典型的「注册时写进 map、请求时查 map 分发」。动态路由是另一条线，下面一题会说到。整体上，路由和中间件都是「注册 + 顺序/查表」，没有依赖反射。

---

### 10：动态路由是怎么实现的？

动态路由我们是用「注册时编译成段、运行时逐段匹配」的方式做的，没有用正则，也没有用 trie。

注册时，如果 path 里包含 `:` 或 `*`，就认为是一条动态路由。先把 path 按 `/` 拆成段，然后压进一个 `DynamicRoute` 结构里：里面存了 method、段的列表、有没有通配符、通配符名字，以及 handler。像 `/users/:id` 就会变成两段 `["users", ":id"]`；像 `/files/*path` 会在遇到 `*path` 时设 `has_wildcard = true`，并且不再往 segments 里加后面的段。所以所有动态路由在启动时就已经编译成「段列表 + 是否带通配符」这种形式了。

匹配时，请求的 path 也按同样规则 `split_path` 成段，然后按注册顺序遍历每一条动态路由。对每条路由先看 method 是否一致，再看段数：没有通配符的话请求段数必须和模式段数一样；有通配符的话请求段数不能少于模式段数。然后逐段比对：模式段是 `:name` 就表示这一段任意都行，把请求里这一段的值记到参数里；是普通字符串就必须完全相等；如果有 `*name`，就把请求 path 里剩余段拼成一条字符串放进参数。第一个匹配成功的就拷贝一份请求、把解析出的参数通过 `set_route_params` 塞进去，然后调这条路由的 handler，并返回，后面的动态路由不再试。所以动态路由就是「预编译的段 + 顺序匹配 + 参数注入」，实现简单、行为也清晰。

---

### 11：HTTP/1.1 和 HTTP/2 的协议是怎么做分发、怎么区分的？

我们服务端是同端口同时支持 H1 和 H2 的，区分方式就是看连接建立后客户端发过来的首包内容，没有用 ALPN（那是 TLS 里的事，我们这里是明文 TCP 的场景）。

具体来说，accept 到一条连接之后，不会立刻交给某个协议会话，而是先做一次「协议探测」：在一个循环里读数据，直到读满 24 字节（HTTP/2 规定的 connection preface 长度）或者中途发现不对、或者对端 EOF。每读一批就检查当前缓冲区：如果已经不再是 H2 preface 的前缀了，就判定为 HTTP/1.1，把已经读到的数据连同协议类型一起返回；如果刚好收满了那 24 字节且内容和标准的 H2 preface 完全一致，就判定为 HTTP/2，同样把首包数据返回。这样既能区分协议，又不会把首包数据丢掉——这些数据会原样交给对应的 H1 或 H2 会话去解析。

判定完之后，根据返回的协议类型创建对应的 session：要么是 `Http1ServerSession`，要么是 `Http2ServerSession`，然后把刚才读到的 initial_data 传进去，后续这条连接上的读写就完全由这个 session 负责了。所以协议分发就是在「每连接、首包」阶段用 H2 的魔数做一次判断，然后分支到不同的会话实现，业务层用的都是同一套 `HttpRequest`/`HttpResponse` 和 `HttpRouter`，不关心底下是 H1 还是 H2。
